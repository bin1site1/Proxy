import re
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import pytz
import jdatetime  # ä¿ç•™åŸä»£ç çš„Jalaliæ—¥æœŸåº“ï¼ˆè™½æœªç›´æ¥ä½¿ç”¨ï¼Œä¿æŒå…¼å®¹æ€§ï¼‰


# -------------------------- 1. ç›®æ ‡TGé¢‘é“åˆ—è¡¨ï¼ˆä½¿ç”¨æ‚¨æä¾›çš„newaddressesï¼‰
newaddresses = [
    "https://t.me/s/vmessiran",
    "https://t.me/s/mrsoulb",
    "https://t.me/s/v2xay",
    "https://t.me/s/vpnaloo",
    "https://t.me/s/v2ray_configs_pool",
    "https://t.me/s/V2RAY_VMESS_free",
    "https://t.me/s/FreakConfig",
    "https://t.me/s/v2rayNG_Matsuri",
    "https://t.me/s/meli_proxyy",
    "https://t.me/s/Daily_Configs",
    "https://t.me/s/customv2ray",
    "https://t.me/s/i10VPN"  # æ¸…é™¤åŸé“¾æ¥æœ«å°¾å¤šä½™ç©ºæ ¼
]
# --------------------------


# -------------------------- 2. å®šä¹‰å»é‡å‡½æ•°ï¼ˆå®Œå…¨å¤ç”¨åŸä»£ç é€»è¾‘ï¼‰
def remove_duplicates(input_list):
    unique_list = []
    for item in input_list:
        if item not in unique_list:
            unique_list.append(item)
    return unique_list
# --------------------------


# -------------------------- 3. éå†é¢‘é“ï¼Œè·å–æ‰€æœ‰ç½‘é¡µHTMLå†…å®¹
html_pages = []
print("å¼€å§‹è·å–TGé¢‘é“é¡µé¢å†…å®¹...")
for idx, url in enumerate(newaddresses, start=1):
    url = url.strip()  # æ¸…é™¤URLé¦–å°¾ç©ºæ ¼
    if not url:
        continue
    try:
        # å¢åŠ User-Agentï¼Œé¿å…è¢«TGæœåŠ¡å™¨å±è”½ï¼ˆåŸä»£ç æœªåŠ ï¼Œæ­¤å¤„ä¼˜åŒ–è¡¥å……ï¼‰
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
            "Accept-Language": "zh-CN,zh;q=0.9"
        }
        # å‘é€GETè¯·æ±‚ï¼ˆè¶…æ—¶15ç§’ï¼Œé¿å…å¡å£³ï¼‰
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()  # è§¦å‘HTTPé”™è¯¯ï¼ˆå¦‚404ã€500ï¼‰
        html_pages.append(response.text)
        print(f"âœ… æˆåŠŸè·å–é¢‘é“ {idx}/{len(newaddresses)}: {url}")
    except Exception as e:
        print(f"âŒ è·å–é¢‘é“ {idx}/{len(newaddresses)} å¤±è´¥: {url} | é”™è¯¯: {str(e)[:30]}")
print(f"\né¡µé¢è·å–å®Œæˆï¼Œå…±æˆåŠŸåŠ è½½ {len(html_pages)} ä¸ªé¢‘é“é¡µé¢\n")
# --------------------------


# -------------------------- 4. å¤šç»´åº¦æå–Telegram Proxyé“¾æ¥ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šé€‚é…Proxyæ ¼å¼ï¼‰
codes = []  # å­˜å‚¨åŸå§‹æå–çš„Proxyé“¾æ¥
print("å¼€å§‹æå–Proxyé“¾æ¥...")
for page_idx, page in enumerate(html_pages, start=1):
    soup = BeautifulSoup(page, 'html.parser')
    code_tags = soup.find_all('code')
    found_any = False  # æ ‡è®°å½“å‰é¡µé¢æ˜¯å¦å·²æå–åˆ°é“¾æ¥

    # ç»´åº¦1ï¼šä»<code>æ ‡ç­¾æå–Proxyï¼ˆå‚è€ƒåŸä»£ç codeæ ‡ç­¾é€»è¾‘ï¼‰
    for code_tag in code_tags:
        code_content = code_tag.text.strip()
        # åŒ¹é…Proxyé“¾æ¥ç‰¹å¾ï¼štg://proxy æˆ– https://t.me/proxy æˆ– /proxyï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
        if any(prefix in code_content for prefix in ["tg://proxy", "https://t.me/proxy", "/proxy?server="]):
            # è¡¥å…¨ç›¸å¯¹è·¯å¾„ï¼š/proxy?server= â†’ https://t.me/proxy?server=
            if code_content.startswith("/proxy?server="):
                code_content = "https://t.me" + code_content
            codes.append(code_content)
            found_any = True

    # ç»´åº¦2ï¼šä»<a>æ ‡ç­¾çš„hrefæå–Proxyï¼ˆå‚è€ƒåŸä»£ç aæ ‡ç­¾é€»è¾‘ï¼‰
    for a in soup.find_all('a', href=True):
        href = a['href'].strip()
        # åŒ¹é…Proxyé“¾æ¥å‰ç¼€ï¼ˆä¸¥æ ¼ç­›é€‰ç›®æ ‡æ ¼å¼ï¼‰
        if href.startswith(("tg://proxy", "https://t.me/proxy", "/proxy?server=")):
            # è¡¥å…¨ç›¸å¯¹è·¯å¾„ + æ¸…é™¤è½¬ä¹‰å­—ç¬¦ï¼ˆå¦‚amp;ï¼‰
            if href.startswith("/proxy?server="):
                href = "https://t.me" + href
            href = href.replace("amp;", "")  # å¤„ç†TGé¡µé¢å¸¸è§çš„è½¬ä¹‰å­—ç¬¦
            codes.append(href)
            found_any = True

    # ç»´åº¦3ï¼šå…¨å±€æ­£åˆ™å…œåº•ï¼ˆå‚è€ƒåŸä»£ç æ­£åˆ™é€»è¾‘ï¼Œé¿å…æ¼æŠ“çº¯æ–‡æœ¬Proxyï¼‰
    if not found_any:
        # æ­£åˆ™è¡¨è¾¾å¼ï¼šåŒ¹é… tg://proxy/... æˆ– https://t.me/proxy/... æˆ– /proxy?server=...
        # ç»ˆæ­¢ç¬¦ï¼šç©ºç™½ã€å¼•å·ã€å°–æ‹¬å·ã€å³æ‹¬å·ï¼ˆé¿å…åŒ¹é…å¤šä½™å†…å®¹ï¼‰
        proxy_pattern = re.compile(
            r'(?:tg://proxy|https://t\.me/proxy|/proxy\?server=)[^\s\'"<>)]+',
            re.IGNORECASE  # å¿½ç•¥å¤§å°å†™ï¼ˆå¦‚TG://PROXYä¹Ÿèƒ½åŒ¹é…ï¼‰
        )
        matches = proxy_pattern.findall(page)
        for match in matches:
            # è¡¥å…¨ç›¸å¯¹è·¯å¾„ + æ¸…é™¤è½¬ä¹‰å­—ç¬¦
            if match.startswith("/proxy?server="):
                match = "https://t.me" + match
            match = match.replace("amp;", "")
            codes.append(match)

    print(f"ğŸ“¥ é¡µé¢ {page_idx}/{len(html_pages)} æå–å®Œæˆï¼Œç´¯è®¡åŸå§‹é“¾æ¥: {len(codes)} æ¡")
print(f"\né“¾æ¥æå–å®Œæˆï¼ŒåŸå§‹é“¾æ¥æ€»æ•°: {len(codes)} æ¡\n")
# --------------------------


# -------------------------- 5. å¤šè½®å»é‡ + è§„èŒƒåŒ–å¤„ç†ï¼ˆå‚è€ƒåŸä»£ç å»é‡é€»è¾‘ï¼‰
print("å¼€å§‹å»é‡å’Œè§„èŒƒåŒ–å¤„ç†...")

# ç¬¬ä¸€è½®å»é‡ï¼šç”¨setå¿«é€Ÿå»é‡ï¼ˆåŸä»£ç é€»è¾‘ï¼‰
codes = list(set(codes))
print(f"ğŸ” ç¬¬ä¸€è½®å»é‡ï¼ˆsetï¼‰å: {len(codes)} æ¡")

# ç¬¬äºŒè½®å»é‡ï¼šç”¨è‡ªå®šä¹‰å‡½æ•°ä¿ç•™é¡ºåºå»é‡ï¼ˆåŸä»£ç é€»è¾‘ï¼‰
processed_codes = remove_duplicates(codes)
print(f"ğŸ” ç¬¬äºŒè½®å»é‡ï¼ˆä¿ç•™é¡ºåºï¼‰å: {len(processed_codes)} æ¡")

# ç¬¬ä¸‰è½®å»é‡ï¼šè§„èŒƒåŒ–é“¾æ¥åå»é‡ï¼ˆå‚è€ƒåŸä»£ç "ç¬¬ä¸‰æ¬¡å»é‡"é€»è¾‘ï¼Œé€‚é…Proxyæ ¼å¼ï¼‰
seen = set()
unique_processed = []
for item in processed_codes:
    # è§„èŒƒåŒ–æ­¥éª¤ï¼š1. æ¸…é™¤é¦–å°¾ç©ºæ ¼ 2. å»é™¤æœ«å°¾æ–œæ  3. ç»Ÿä¸€åè®®æ ¼å¼
    norm = item.strip()
    norm = norm.rstrip('/')  # å»é™¤æœ«å°¾å¤šä½™æ–œæ ï¼ˆå¦‚ tg://proxy/ â†’ tg://proxyï¼‰
    norm = norm.replace("amp;", "")  # å†æ¬¡æ¸…é™¤è½¬ä¹‰å­—ç¬¦ï¼ˆåŒé‡ä¿é™©ï¼‰
    
    # å¤„ç†ç‰¹æ®Šæƒ…å†µï¼šç¡®ä¿é“¾æ¥ä»¥ç›®æ ‡åè®®å¼€å¤´
    if not norm.lower().startswith(("tg://proxy", "https://t.me/proxy")):
        # é‡æ–°åŒ¹é…åè®®ä½ç½®ï¼Œæˆªå–æ­£ç¡®é“¾æ¥ï¼ˆé¿å…æ®‹ç•™å¤šä½™å‰ç¼€ï¼‰
        for proto in ("tg://proxy", "https://t.me/proxy"):
            if proto in norm.lower():
                idx = norm.lower().find(proto)
                norm = norm[idx:]  # ä»åè®®å¼€å¤´æˆªå–
                break
    
    # å»é‡ï¼šä»…ä¿ç•™æœªå‡ºç°è¿‡çš„è§„èŒƒåŒ–é“¾æ¥
    if norm not in seen:
        seen.add(norm)
        unique_processed.append(item)  # ä¿ç•™åŸå§‹é“¾æ¥ï¼ˆä»…ç”¨normå»é‡ï¼‰

processed_codes = unique_processed
print(f"ğŸ” ç¬¬ä¸‰è½®å»é‡ï¼ˆè§„èŒƒåŒ–ï¼‰å: {len(processed_codes)} æ¡")

# æœ€ç»ˆè¿‡æ»¤ï¼šç¡®ä¿æ‰€æœ‰é“¾æ¥éƒ½æ˜¯ç›®æ ‡æ ¼å¼ï¼ˆåŒé‡æ ¡éªŒï¼Œé¿å…æ®‹ç•™æ— æ•ˆé“¾æ¥ï¼‰
final_proxies = []
for link in processed_codes:
    link = link.strip().replace("amp;", "")
    if link.startswith(("tg://proxy", "https://t.me/proxy")):
        final_proxies.append(link)
print(f"âœ… æœ€ç»ˆæœ‰æ•ˆProxyé“¾æ¥æ•°: {len(final_proxies)} æ¡\n")
# --------------------------


# -------------------------- 6. ä¿å­˜åˆ°proxylist.txtï¼ˆæŒ‰è¦æ±‚æ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªï¼Œç©ºè¡Œéš”å¼€ï¼‰
print("å¼€å§‹ä¿å­˜åˆ° proxylist.txt...")
if not final_proxies:
    print("âŒ æ— æœ‰æ•ˆProxyé“¾æ¥ï¼Œæ— éœ€ä¿å­˜")
else:
    # è·å–ä¸Šæµ·æ—¶åŒºå½“å‰æ—¶é—´ï¼ˆå‚è€ƒåŸä»£ç æ—¶é—´å¤„ç†é€»è¾‘ï¼‰
    current_date_time = datetime.now(pytz.timezone('Asia/Shanghai'))
    final_string = current_date_time.strftime("%mæœˆ%dæ—¥ | %H:%M")  # ä¸­æ–‡æ—¶é—´æ ¼å¼
    final_others_string = current_date_time.strftime("%mæœˆ%dæ—¥")

    # å†™å…¥æ–‡ä»¶ï¼ˆç¼–ç UTF-8ï¼Œé¿å…ä¸­æ–‡ä¹±ç ï¼‰
    with open("proxylist.txt", "w", encoding="utf-8") as file:
        # å†™å…¥å¤´éƒ¨æ³¨é‡Šï¼ˆå‚è€ƒåŸä»£ç æ³¨é‡Šé€»è¾‘ï¼Œæ–¹ä¾¿è¯†åˆ«ï¼‰
        file.write(f"# Telegram Proxy æŠ“å–ç»“æœ\n")
        file.write(f"# æŠ“å–æ—¶é—´: {final_string}ï¼ˆä¸Šæµ·æ—¶åŒºï¼‰\n")
        file.write(f"# æŠ“å–é¢‘é“æ•°: {len(newaddresses)} ä¸ª\n")
        file.write(f"# æœ‰æ•ˆProxyæ•°: {len(final_proxies)} æ¡\n")
        file.write(f"# æ ¼å¼: æ¯è¡Œä¸€ä¸ªé“¾æ¥ï¼Œè¡Œä¸è¡Œç©ºè¡Œéš”å¼€\n")
        file.write("-" + "-"*50 + "\n\n")  # åˆ†éš”çº¿

        # æŒ‰è¦æ±‚å†™å…¥é“¾æ¥ï¼šæ¯è¡Œä¸€ä¸ªï¼Œè¡Œä¸è¡Œç©ºè¡Œéš”å¼€
        for proxy in final_proxies:
            file.write(f"{proxy}\n")  # å†™å…¥é“¾æ¥
            file.write("\n")  # ç©ºè¡Œï¼ˆè¡Œä¸è¡Œéš”å¼€ï¼‰

    print(f"âœ… ä¿å­˜æˆåŠŸï¼æ–‡ä»¶è·¯å¾„: {os.path.abspath('proxylist.txt')}")
    print(f"ğŸ“„ æ–‡ä»¶æ ¼å¼ï¼šæ¯è¡Œ1ä¸ªProxyé“¾æ¥ï¼Œè¡Œä¸è¡Œç©ºè¡Œéš”å¼€")

print("\n" + "="*50)
print("ğŸ‰ æ‰€æœ‰æ“ä½œå®Œæˆï¼")
print(f"ğŸ“Š æœ€ç»ˆç»“æœï¼šå…±æŠ“å– {len(final_proxies)} æ¡æœ‰æ•ˆTelegram Proxyé“¾æ¥")
print(f"ğŸ“ ä¿å­˜æ–‡ä»¶ï¼šproxylist.txtï¼ˆå½“å‰ç›®å½•ï¼‰")
print("-"*50)