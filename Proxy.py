import re
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import pytz
import urllib.parse  # ç”¨äºURLè§£ç å’Œè§£æï¼ˆè§£å†³è½¬ä¹‰å­—ç¬¦ã€ç¼–ç é—®é¢˜ï¼‰
# jdatetime æœªå®é™…ä½¿ç”¨ï¼Œä¿ç•™æ³¨é‡Šè¯´æ˜ï¼ˆé¿å…å†—ä½™ä¾èµ–ï¼‰
# import jdatetime  


# -------------------------- æ ¸å¿ƒé…ç½®ï¼ˆæ— ä»£ç†ç›¸å…³é¡¹ï¼Œç”¨æˆ·å¯ä¿®æ”¹ï¼‰
# 1. ç›®æ ‡TGé¢‘é“åˆ—è¡¨ï¼ˆä¿æŒåŸåˆ—è¡¨ï¼‰
TG_CHANNELS = [
    "https://t.me/s/gaosuwang",
    "https://t.me/s/ProxyMTProting",
    "https://t.me/s/hgwzcd",
    "https://t.me/s/GSDL6",
    "https://t.me/s/changanhutui",
    "https://t.me/s/qiuyue2",
    "https://t.me/s/gsdl01",
    "https://t.me/s/juzibaipiao",
    "https://t.me/s/daili81",
    "https://t.me/s/hbgzs1",
    "https://t.me/s/VPNzhilian",
    "https://t.me/s/duxiangdail",
    "https://t.me/s/XB811",
    "https://t.me/s/ngg789",
    "https://t.me/s/TGTW88",
    "https://t.me/s/afeiSSS",
    "https://t.me/s/feijidailil",
    "https://t.me/s/dail99",
]

# 2. æŠ“å–é…ç½®ï¼ˆè¶…æ—¶æ—¶é—´ã€ç›®æ ‡Proxyæ ¼å¼ï¼‰
TIMEOUT = 15  # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆé¿å…å¡æ­»åå°ï¼‰
PROXY_PREFIXES = ("tg://proxy", "https://t.me/proxy", "/proxy?server=")  # ç›®æ ‡Proxyå‰ç¼€
OUTPUT_FILE = "proxylist.txt"  # è¾“å‡ºæ–‡ä»¶å
# --------------------------


# -------------------------- å·¥å…·å‡½æ•°ï¼ˆå»é‡ã€URLå¤„ç†ï¼‰
def remove_duplicates(input_list):
    """ä¿ç•™é¡ºåºçš„å»é‡å‡½æ•°ï¼ˆå¢åŠ ç±»å‹æ ¡éªŒï¼Œé¿å…æ— æ•ˆæ•°æ®ï¼‰"""
    unique_list = []
    for item in input_list:
        if isinstance(item, str) and item.strip() not in unique_list:
            unique_list.append(item.strip())
    return unique_list


def normalize_proxy_url(url):
    """æ ‡å‡†åŒ–Proxy URLï¼ˆè§£å†³è½¬ä¹‰ã€è·¯å¾„è¡¥å…¨ã€æ ¼å¼ç»Ÿä¸€é—®é¢˜ï¼‰"""
    if not isinstance(url, str):
        return ""
    
    # æ­¥éª¤1ï¼šæ¸…é™¤ç©ºæ ¼ + URLè§£ç ï¼ˆå¤„ç†%26ã€amp;ç­‰è½¬ä¹‰å­—ç¬¦ï¼‰
    normalized = url.strip()
    normalized = urllib.parse.unquote(normalized)  # è§£ç URLç¼–ç å­—ç¬¦
    normalized = normalized.replace("amp;", "")    # å¤„ç†TGå¸¸è§çš„amp;è½¬ä¹‰
    
    # æ­¥éª¤2ï¼šè¡¥å…¨ç›¸å¯¹è·¯å¾„ï¼ˆ/proxy?server= â†’ https://t.me/proxy?server=ï¼‰
    if normalized.startswith("/proxy?server="):
        normalized = "https://t.me" + normalized
    
    # æ­¥éª¤3ï¼šç»Ÿä¸€æ ¼å¼ï¼ˆå»é™¤æœ«å°¾æ–œæ ã€åè®®å°å†™ï¼‰
    normalized = normalized.rstrip("/")  # é¿å… tg://proxy/ å’Œ tg://proxy è¢«åˆ¤å®šä¸ºä¸åŒ
    parsed = urllib.parse.urlparse(normalized)
    if parsed.scheme:
        normalized = normalized.replace(parsed.scheme, parsed.scheme.lower())  # åè®®å°å†™ï¼ˆå¦‚TG:// â†’ tg://ï¼‰
    
    # æ­¥éª¤4ï¼šæ ¡éªŒæ˜¯å¦ä¸ºæœ‰æ•ˆProxyæ ¼å¼
    return normalized if normalized.startswith(PROXY_PREFIXES) else ""
# --------------------------


# -------------------------- æ ¸å¿ƒæŠ“å–é€»è¾‘ï¼ˆç§»é™¤ä»£ç†å‚æ•°ï¼Œä¿ç•™å¼‚å¸¸å¤„ç†ï¼‰
def fetch_tg_proxies(channels, timeout=10):
    """
    æŠ“å–TGé¢‘é“ä¸­çš„Proxyé“¾æ¥ï¼ˆæ— ä»£ç†ç‰ˆæœ¬ï¼‰
    :param channels: TGé¢‘é“URLåˆ—è¡¨
    :param timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´
    :return: åŸå§‹Proxyåˆ—è¡¨ï¼ˆæœªå»é‡ï¼‰
    """
    raw_proxies = []
    total_channels = len(channels)
    success_count = 0  # ç»Ÿè®¡æˆåŠŸè¯·æ±‚çš„é¢‘é“æ•°

    print(f"å¼€å§‹æŠ“å– {total_channels} ä¸ªTGé¢‘é“ï¼ˆæ— ä»£ç†æ¨¡å¼ï¼‰...\n")

    for idx, channel_url in enumerate(channels, 1):
        print(f"[{idx}/{total_channels}] æ­£åœ¨è¯·æ±‚ï¼š{channel_url}")
        try:
            # å‘èµ·è¯·æ±‚ï¼ˆç§»é™¤proxieså‚æ•°ï¼Œä¿ç•™SSLè­¦å‘Šç¦ç”¨å’Œè¶…æ—¶ï¼‰
            requests.packages.urllib3.disable_warnings()  # è§£å†³TG SSLè¯ä¹¦å¯èƒ½çš„è­¦å‘Š
            response = requests.get(
                url=channel_url,
                timeout=timeout,
                verify=False  # éƒ¨åˆ†TGé•œåƒå¯èƒ½å­˜åœ¨SSLè¯ä¹¦é—®é¢˜ï¼Œä¸´æ—¶ç¦ç”¨
            )
            response.raise_for_status()  # è§¦å‘HTTPé”™è¯¯ï¼ˆå¦‚404ã€500ï¼‰
            success_count += 1

        except requests.exceptions.ConnectionError:
            print(f"âŒ è¯·æ±‚å¤±è´¥ï¼šæ— æ³•è¿æ¥åˆ°é¢‘é“ï¼ˆæ³¨æ„ï¼šå›½å†…æ— ä»£ç†ç¯å¢ƒå¯èƒ½æ— æ³•è®¿é—®TGï¼‰\n")
            continue
        except requests.exceptions.Timeout:
            print(f"âŒ è¯·æ±‚å¤±è´¥ï¼šè¶…æ—¶ï¼ˆå»ºè®®è°ƒæ•´TIMEOUTå‚æ•°ï¼‰\n")
            continue
        except requests.exceptions.HTTPError as e:
            print(f"âŒ è¯·æ±‚å¤±è´¥ï¼šHTTPé”™è¯¯ {e.response.status_code}ï¼ˆé¢‘é“å¯èƒ½ä¸å­˜åœ¨æˆ–å·²å¤±æ•ˆï¼‰\n")
            continue
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¤±è´¥ï¼šæœªçŸ¥é”™è¯¯ {str(e)}\n")
            continue

        # è§£æHTMLï¼Œæå–Proxy
        soup = BeautifulSoup(response.text, "lxml" if "lxml" in BeautifulSoup.__modules__ else "html.parser")
        found_in_channel = False

        # ç»´åº¦1ï¼šä»<code>æ ‡ç­¾æå–ï¼ˆä¼˜å…ˆï¼ŒProxyå¸¸æ”¾åœ¨ä»£ç å—ä¸­ï¼‰
        for code_tag in soup.find_all("code"):
            code_text = code_tag.text.strip()
            normalized = normalize_proxy_url(code_text)
            if normalized and normalized not in raw_proxies:
                raw_proxies.append(normalized)
                found_in_channel = True

        # ç»´åº¦2ï¼šä»<a>æ ‡ç­¾çš„hrefæå–ï¼ˆé“¾æ¥å½¢å¼çš„Proxyï¼‰
        for a_tag in soup.find_all("a", href=True):
            href = a_tag["href"].strip()
            normalized = normalize_proxy_url(href)
            if normalized and normalized not in raw_proxies:
                raw_proxies.append(normalized)
                found_in_channel = True

        # ç»´åº¦3ï¼šå…¨å±€æ­£åˆ™å…œåº•ï¼ˆçº¯æ–‡æœ¬ä¸­éšè—çš„Proxyï¼‰
        if not found_in_channel:
            proxy_pattern = re.compile(
                r'(tg://proxy|https://t\.me/proxy|/proxy\?server=)[^\s\'"<>)]+',
                re.IGNORECASE | re.MULTILINE
            )
            matches = proxy_pattern.findall(response.text)
            for match in matches:
                normalized = normalize_proxy_url(match)
                if normalized and normalized not in raw_proxies:
                    raw_proxies.append(normalized)
                    found_in_channel = True

        print(f"âœ… {'å·²æ‰¾åˆ°Proxy' if found_in_channel else 'æœªæ‰¾åˆ°Proxy'}ï¼ˆå½“å‰ç´¯è®¡ï¼š{len(raw_proxies)} æ¡ï¼‰\n")

    print(f"æŠ“å–å®Œæˆï¼š{success_count}/{total_channels} ä¸ªé¢‘é“è¯·æ±‚æˆåŠŸï¼Œå…±æŠ“å– {len(raw_proxies)} æ¡åŸå§‹Proxy")
    return raw_proxies
# --------------------------


# -------------------------- æ•°æ®å¤„ç†ä¸æ–‡ä»¶å†™å…¥ï¼ˆç§»é™¤ä»£ç†ç›¸å…³æ ¡éªŒï¼‰
def process_and_save_proxies(raw_proxies):
    """å¤„ç†ï¼ˆå»é‡ã€è¿‡æ»¤ï¼‰Proxyå¹¶å†™å…¥æ–‡ä»¶"""
    # ç¬¬ä¸€è½®å»é‡ï¼šä¿ç•™é¡ºåºå»é‡
    unique_proxies = remove_duplicates(raw_proxies)
    # ç¬¬äºŒè½®è¿‡æ»¤ï¼šç¡®ä¿æœ€ç»ˆéƒ½æ˜¯æœ‰æ•ˆProxyæ ¼å¼
    final_proxies = [p for p in unique_proxies if p.startswith(PROXY_PREFIXES)]

    # è·å–ä¸Šæµ·æ—¶åŒºæ—¶é—´ï¼ˆå®¹é”™å¤„ç†ï¼‰
    try:
        sh_tz = pytz.timezone("Asia/Shanghai")
        current_time = datetime.now(sh_tz).strftime("%Yå¹´%mæœˆ%dæ—¥ | %H:%M:%S")
    except pytz.UnknownTimeZoneError:
        current_time = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ | %H:%M:%S")
        print("âš ï¸  ä¸Šæµ·æ—¶åŒºæœªæ‰¾åˆ°ï¼Œä½¿ç”¨ç³»ç»Ÿé»˜è®¤æ—¶åŒº")

    # å†™å…¥æ–‡ä»¶ï¼ˆä¿ç•™æƒé™å¼‚å¸¸å¤„ç†ï¼‰
    try:
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            # å¤´éƒ¨æ³¨é‡Šï¼ˆè¡¥å……æ— ä»£ç†è¯´æ˜ï¼‰
            f.write(f"# TG Proxy æŠ“å–ç»“æœï¼ˆæ— ä»£ç†æ¨¡å¼ï¼‰\n")
            f.write(f"# æŠ“å–æ—¶é—´ï¼š{current_time}ï¼ˆä¸Šæµ·æ—¶åŒºï¼‰\n")
            f.write(f"# ç›®æ ‡é¢‘é“æ•°ï¼š{len(TG_CHANNELS)} ä¸ª\n")
            f.write(f"# æˆåŠŸè¯·æ±‚æ•°ï¼š{len([c for c in TG_CHANNELS if _check_channel_accessible(c)]):d} ä¸ªï¼ˆå®æ—¶æ ¡éªŒï¼‰\n")
            f.write(f"# åŸå§‹æŠ“å–æ•°ï¼š{len(raw_proxies)} æ¡\n")
            f.write(f"# æœ€ç»ˆæœ‰æ•ˆæ•°ï¼š{len(final_proxies)} æ¡\n")
            f.write(f"# æ³¨æ„ï¼šæ— ä»£ç†æ¨¡å¼ä¸‹ï¼Œå›½å†…ç¯å¢ƒå¯èƒ½æ— æ³•è®¿é—®TGï¼Œå¯¼è‡´æŠ“å–å¤±è´¥\n")
            f.write(f"# --------------------------\n\n")

            # å†™å…¥Proxyï¼ˆæ¯è¡Œ1ä¸ªï¼Œç©ºè¡Œåˆ†éš”ï¼‰
            if final_proxies:
                for idx, proxy in enumerate(final_proxies, 1):
                    f.write(f"{idx}. {proxy}\n\n")
            else:
                f.write("# æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆProxyï¼ˆå¯èƒ½åŸå› ï¼šé¢‘é“æ— æ•°æ®ã€æ ¼å¼å˜æ›´ï¼Œæˆ–æ— ä»£ç†æ— æ³•è®¿é—®TGï¼‰\n")

        print(f"\nâœ… æ–‡ä»¶å†™å…¥æˆåŠŸï¼è·¯å¾„ï¼š{OUTPUT_FILE}")
        print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡ï¼š{len(final_proxies)} æ¡æœ‰æ•ˆProxy")

    except PermissionError:
        print(f"âŒ æ–‡ä»¶å†™å…¥å¤±è´¥ï¼šæ— æƒé™ï¼ˆè¯·å°†ä»£ç ç§»åˆ°æ¡Œé¢/æ–‡æ¡£ç­‰éç³»ç»Ÿç›®å½•ï¼‰")
    except Exception as e:
        print(f"âŒ æ–‡ä»¶å†™å…¥å¤±è´¥ï¼šæœªçŸ¥é”™è¯¯ {str(e)}")


def _check_channel_accessible(channel_url):
    """è¾…åŠ©å‡½æ•°ï¼šæ£€æŸ¥é¢‘é“æ˜¯å¦å¯è®¿é—®ï¼ˆæ— ä»£ç†ï¼‰"""
    try:
        requests.packages.urllib3.disable_warnings()
        response = requests.get(channel_url, timeout=5, verify=False)
        return response.status_code == 200
    except:
        return False
# --------------------------


# -------------------------- ä¸»å‡½æ•°ï¼ˆç§»é™¤ä»£ç†ä¾èµ–æ£€æµ‹ï¼‰
if __name__ == "__main__":
    # 1. ç¼ºå¤±ä¾èµ–æ£€æµ‹ï¼ˆç§»é™¤requests[socks]ï¼Œä»…ä¿ç•™æ ¸å¿ƒä¾èµ–ï¼‰
    required_packages = ["requests", "beautifulsoup4", "pytz", "lxml"]
    missing_packages = []
    for pkg in required_packages:
        try:
            __import__(pkg)
        except ImportError:
            missing_packages.append(pkg)
    
    if missing_packages:
        print(f"âš ï¸  æ£€æµ‹åˆ°ç¼ºå¤±ä¾èµ–ï¼š{', '.join(missing_packages)}")
        print(f"è¯·å…ˆæ‰§è¡Œå‘½ä»¤å®‰è£…ï¼špip install {' '.join(missing_packages)}")
        exit()

    # 2. æ‰§è¡ŒæŠ“å–ã€å¤„ç†ã€å†™å…¥ï¼ˆæ— ä»£ç†è°ƒç”¨ï¼‰
    raw_proxies = fetch_tg_proxies(
        channels=TG_CHANNELS,
        timeout=TIMEOUT
    )
    process_and_save_proxies(raw_proxies)