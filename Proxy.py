import re

import requests  # å¯¼å…¥requestsåº“ï¼Œç”¨äºå‘é€HTTPè¯·æ±‚
from bs4 import BeautifulSoup  # å¯¼å…¥BeautifulSoupåº“ï¼Œç”¨äºè§£æHTML
from datetime import datetime, timezone  # å¯¼å…¥datetimeåº“ï¼Œç”¨äºå¤„ç†æ—¥æœŸå’Œæ—¶é—´
import pytz  # å¯¼å…¥pytzåº“ï¼Œç”¨äºå¤„ç†æ—¶åŒº
import jdatetime  # å¯¼å…¥jdatetimeåº“ï¼Œç”¨äºå¤„ç†Jalaliæ—¥æœŸ


newaddresses = [
    "https://t.me/s/vmessiran",
    "https://t.me/s/mrsoulb",
    "https://t.me/s/v2xay",
    "https://t.me/s/vpnaloo",
    "https://t.me/s/v2ray_configs_pool",
    "https://t.me/s/V2RAY_VMESS_free",
    "https://t.me/s/FreakConfig",
    "https://t.me/s/v2rayNG_Matsuri",
    "https://t.me/s/meli_proxyy",
    "https://t.me/s/Daily_Configs",
    "https://t.me/s/customv2ray",
    "https://t.me/s/i10VPN ",


]
# å®šä¹‰å»é‡å‡½æ•°ï¼Œè¾“å…¥åˆ—è¡¨ï¼Œè¿”å›å»é‡åçš„åˆ—è¡¨
def remove_duplicates(input_list):
    unique_list = []
    for item in input_list:
        if item not in unique_list:
            unique_list.append(item)
    return unique_list

html_pages = []  # ç”¨äºå­˜å‚¨æ¯ä¸ªç½‘é¡µçš„HTMLå†…å®¹

# éå†æ‰€æœ‰åœ°å€ï¼Œè·å–ç½‘é¡µå†…å®¹
for url in newaddresses:
    response = requests.get(url)  # å‘é€GETè¯·æ±‚
    html_pages.append(response.text)  # ä¿å­˜ç½‘é¡µå†…å®¹

codes = []  # ç”¨äºå­˜å‚¨æ‰€æœ‰æŠ“å–åˆ°çš„é…ç½®ä»£ç 

# éå†æ‰€æœ‰HTMLé¡µé¢ï¼Œè§£æå¹¶æå–codeæ ‡ç­¾å†…å®¹
for page in html_pages:
    soup = BeautifulSoup(response.content, 'html.parser')
    links = soup.find_all('a', href=True)

    found_in_channel = 0
    for link in links:
        href = link.get('href').replace('amp;', '') if link.get('href') else ""
        if any(href.startswith(prefix) for prefix in [
                        "https://t.me/proxy?server=",
                        "tg://proxy?server=",
                        "/proxy?server=",
                        "https://t.me/s/proxy?server="
        ]):
            if href.startswith("/"):
                href = "https://t.me" + href
            self.extracted_links.add(href)
            found_in_channel += 1

codes = list(set(codes))  # å»é‡

processed_codes = []  # ç”¨äºå­˜å‚¨å¤„ç†åçš„é…ç½®

# è·å–å½“å‰æ—¶é—´ï¼ˆä¸Šæµ·æ—¶åŒºï¼‰
current_date_time = datetime.now(pytz.timezone('Asia/Shanghai'))
current_month = current_date_time.strftime("%m")  # æœˆä»½ï¼ˆæ•°å­—ï¼‰
current_day = current_date_time.strftime("%d")    # æ—¥æœŸ
updated_hour = current_date_time.strftime("%H")   # å°æ—¶
updated_minute = current_date_time.strftime("%M") # åˆ†é’Ÿ
final_string = f"{current_month}æœˆ{current_day}æ—¥ | {updated_hour}:{updated_minute}"  # ä¸­æ–‡æ ¼å¼æ—¶é—´å­—ç¬¦ä¸²
final_others_string = f"{current_month}æœˆ{current_day}æ—¥"  # ä»…æ—¥æœŸå­—ç¬¦ä¸²
config_string = "#âœ… " + str(final_string) + "-"  # é…ç½®å¤´éƒ¨å­—ç¬¦ä¸²

processed_codes = remove_duplicates(processed_codes)  # å†æ¬¡å»é‡

new_processed_codes = []  # ç”¨äºå­˜å‚¨æœ€ç»ˆå¤„ç†åçš„é…ç½®

i = 0  # åˆå§‹åŒ–æœåŠ¡å™¨è®¡æ•°å™¨
with open("config.txt", "w", encoding="utf-8") as file:  # ä»¥å†™å…¥æ¨¡å¼æ‰“å¼€æ–‡ä»¶
    for code in new_processed_codes:
        if i == 0:
            config_string = "#ğŸŒå·²æ›´æ–°äº" + config_string + " | æ¯15åˆ†é’Ÿæ›´æ–°ä¸€æ¬¡"  # ç¬¬ä¸€è¡Œå†™æ›´æ–°æ—¶é—´
        else:
            config_string = "#ğŸŒæœåŠ¡å™¨" + str(i) + " | " + str(final_others_string) + " |bin1site1.github.io "  # å…¶ä»–è¡Œå†™æœåŠ¡å™¨ç¼–å·å’Œæ—¥æœŸ
        config_final = code + config_string  # æ‹¼æ¥é…ç½®å’Œæ³¨é‡Š
        file.write(config_final + "\n")  # å†™å…¥æ–‡ä»¶å¹¶æ¢è¡Œ
        i += 1  # æœåŠ¡å™¨è®¡æ•°å™¨åŠ ä¸€
